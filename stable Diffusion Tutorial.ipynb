{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Basic Tutorial of Stable Diffusion with MathðŸ“ and CodeðŸ’»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As learners like you, when we first touch upon the state-of-art model of Stable Diffusion, we found it complicated and perplexing. Based of empirical results and combined with multiple distinct parts, the model itself is hard to appreciate and even hard to follow without a clear and logical pathðŸ˜–. If you feel the same way, don't worry, we were all at the same boatðŸ˜¤. As learners, we looked through different materials and tutorials, but we could not find something that both comprehensive enough logic-wise or coding-wiseðŸ˜”.\n",
    "\n",
    "Therefore, we aim to create a natural and straightforward introduction in both the foundation and implementation of the modelðŸ¤—.\n",
    "\n",
    "So let's begin our journey!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Stable Diffusion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Wikipedia and its github page, Stable Diffusion is a deep learning, text-to-image model released in 2022 based on diffusion techniques. From this, we know its backbone is essentially a model called diffusion. So, before we dive into the actual stable diffusion, we think it's better to learn about what is Diffusion. Or more specifically, DDPM - Denoising Diffusion Probabilistic Models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising Diffusion Probabilistic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are intereted in reading the original paper of DDPM by Ho et.al published 2020, here is the link for the paper: https://arxiv.org/pdf/2006.11239.pdf.\n",
    "\n",
    "However, the original paper is really math-heavy, and skipped a lot of details. Thus, we provide our understanding and visions which should be slighly easier (?) to comprehend.\n",
    "\n",
    "In short, the ddpm (diffusion) model is basically a process of adding noise and denoising, which could be easliy divided into 2 seperate directions, the forward process - adding noise and the reverse process - denosing. The general idea behind this model is, if by defined, we know how to iteratively add noise to a image that let it eventually become a completely noisy image (in a sense equals to random samples), could we use some deep learning networks to learn the noise added to the model in each step?\n",
    "\n",
    "And if we could accomplish that, does it mean that we have a way to predict the added noise for each state during the process (the difference of noise level between each timestep)? Then using the noise we predicted, does it mean that we could start from a random noise, and continuing substract the noise we predicted, which at the end give us a image without noise? That is the generative result we want. \n",
    "\n",
    "Above is a very brief introduciton/ intuition for you to have something in mind before we head into the detail maths. But maths is something that we not able to avoid. I'll start with the forward process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Process - Adding Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](forward_pass.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the forward process, the paper defined $X_t = \\sqrt{a_t}X_{t-1} + \\sqrt{1-a_t}Z_t$, \n",
    "    - where $X_t$ is the image at timestep $t$, after added noise t times. \n",
    "    - $Z_t$ is the noise added from Normal Distribution ~ N(0, 1)\n",
    "    - Yes, we need to also explain what timestep $t$ means. It is the number of steps we take to reach the final noisy image, shown in the figure above, timestep is the underscore number below each state of adding noise to the previous image.  \n",
    "    - $a_t$ is the noise level at timestep $t$, and it is defined by using a function $a_t = 1 - \\beta_t$ where $\\beta_t$ is a hyperparameter that increases through timesteps, which t increases $\\beta_t$ also increase. This would cause $a_t$ to decrease through timesteps.\n",
    "        - So why? Why we defined $a_t$ in this way? Observe the formula, when $a_t$ decreases through timestep, $\\sqrt{1-a_t}$ would conversely increase. This means that the noise added to the image would increase through timesteps. What it means? Imagine we start from a clean image, and we add noise to it, even a little bit of noise would be noticeable. But if we add noise to a noisy image, the noise added would be insignificant. As a result, with more timesteps, we also need to add more noise to make it be more inflential. This is the logic behind the definition of $a_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- However, we don't want to add noise actually timestep by timestep, we want to add noise to the image in one go. If we have our image $X_0$, we want to have $X_T$ immediately. So now we want to relate $X_T$ to $X_0$ directly. \n",
    "    - We can do this by expanding the formula $X_t = \\sqrt{a_t}X_{t-1} + \\sqrt{1-a_t}Z_t$. \n",
    "    - => $X_t = \\sqrt{a_t}(\\sqrt{a_{t-1}}X_{t-2} + \\sqrt{1-a_{t-1}}Z_{t-1}) + \\sqrt{1-a_t}Z_t$\n",
    "    - => $X_t = \\sqrt{a_ta_{t-1}}X_{t-2} + \\sqrt{a_t(1-a_{t-1})}Z_{t-1} + \\sqrt{1-a_t}Z_t$\n",
    "    - For Gaussian distribution, if we multiply two Gaussian distribution, in this case $Z_t$ and $Z_{t-1}$ \n",
    "        - if we multiply a Gaussian distribution with constant $c$: new $\\mu' = c\\mu$, new $\\sigma' = |c|\\sigma => \\sigma'^2 = |c|\\sigma^2$\n",
    "        - $\\sqrt{a_t(1-a_{t-1})}Z_{t-1} \\sim N(0, a_t(1-a_{t-1}))$ and $\\sqrt{1-a_t}Z_t \\sim N(0, 1-a_t)$ are still Gaussian distributions\n",
    "        - for any Guassion distribution $N(\\mu_1, \\sigma_1^2)$ and $N(\\mu_2, \\sigma_2^2)$, if they are independent, then $N(\\mu_1, \\sigma_1^2) + N(\\mu_2, \\sigma_2^2) \\sim N(\\mu_1 + \\mu_2, \\sigma_1^2 + \\sigma_2^2)$\n",
    "    - => $X_t = \\sqrt{a_ta_{t-1}}X_{t-2} + \\sqrt{1 - a_t + a_t -a_ta_{t-1}}\\bar Z$ where $\\bar Z \\sim N(0, 1)$\n",
    "    - => $X_t = \\sqrt{a_ta_{t-1}}X_{t-2} + \\sqrt{1 - a_ta_{t-1}}\\bar Z$\n",
    "    - continue this process, we can get $X_T = \\sqrt{a_ta_{t-1}...a_1}X_0 + \\sqrt{1 - a_ta_{t-1}...a_1}\\bar Z$\n",
    "- So if let $\\bar a_t = \\prod_1^t a_i$, \n",
    "- => $X_t = \\sqrt{\\bar a_t}X_0 + \\sqrt{1 - \\bar a_t}\\bar Z$\n",
    "\n",
    "And this is our final equation for adding noise in the forward process. It's possible you find the math above is a bit hard to follow, you should remember this formula, and we will use it in actual implementation.\n",
    "This concludes the forward process. Hurrah! We are halfway there! ðŸ˜€ \n",
    "\n",
    "Next, we will move on to the reverse process - denoising."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverse Process - Denoising\n",
    "\n",
    "![](backward_pass.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name suggests, the reverse process is the opposite of the forward process. In the forward process, we add noise to the image, and in the reverse process, we remove the noise from the image. So if we are given the noisy image $X_t$, how can we know the less noisy image $X_{t-1}$?\n",
    "\n",
    "Basically, we want the probability $q(X_{t-1}|X_t)$ But now, we only know how to get $p(X_t|X_{t-1})$ which is the forward process. So how can we get the reverse process? How can we related conditional probability if we know one direction?\n",
    "\n",
    "Yes! You are right!ðŸ˜Ž We can use the Bayes' Theorem! But with a little trick by including $X_0$.\n",
    "\n",
    "- So now, we want to get $q(X_{t-1}|X_t, X_0)$, and \n",
    "    - $q(X_{t-1}|X_t, X_0) = \\frac{q(X_t|X_{t-1}, X_0)q(X_{t-1}|X_0)}{q(X_t|X_0)}$\n",
    "    - Notice that all the terms in the right-hand side are known from the forward process\n",
    "        - a little reminder: we got\n",
    "            -  $X_t = \\sqrt{a_t}X_{t-1} + \\sqrt{1-a_t}Z_t$\n",
    "            - $X_t = \\sqrt{\\bar a_t}X_0 + \\sqrt{1 - \\bar a_t}\\bar Z$\n",
    "    - $q(X_t|X_{t-1}, X_0) = \\sqrt{a_t}X_{t-1} + \\sqrt{1-a_t}Z_t$, which $ \\sim N(\\sqrt{a_t}X_{t-1}, 1-a_t)$\n",
    "    - $q(X_{t-1}|X_0) = \\sqrt{\\bar a_{t-1}}X_0 + \\sqrt{1 - \\bar a_{t-1}}\\bar Z$, which $ \\sim N(\\sqrt{\\bar a_{t-1}}X_0, 1-\\bar a_{t-1})$\n",
    "    - $q(X_t|X_0) = \\sqrt{\\bar a_t}X_0 + \\sqrt{1 - \\bar a_t}\\bar Z$, which $ \\sim N(\\sqrt{\\bar a_t}X_0, 1-\\bar a_t)$\n",
    "- Now if we only consider $q(X_t|X_{t-1}, X_0)q(X_{t-1}|X_0)$\n",
    "    - $q(X_t|X_{t-1}, X_0)q(X_{t-1}|X_0) = N(\\sqrt{a_t}X_{t-1}, 1-a_t)N(\\sqrt{\\bar a_{t-1}}X_0, 1-\\bar a_{t-1})$\n",
    "    - = $\\frac{1}{\\sigma_1 \\sqrt{2\\pi}}e^{-\\frac{1}{2} (\\frac{x - \\mu_1}{\\sigma_1})^2} \\cdot \\frac{1}{\\sigma_2 \\sqrt{2\\pi}}e^{-\\frac{1}{2} (\\frac{x - \\mu_2}{\\sigma_2})^2}$\n",
    "    - = $\\frac{1}{\\sigma_1 \\sigma_2 \\sqrt{2\\pi}}e^{-\\frac{1}{2} (\\frac{x - \\mu_1}{\\sigma_1})^2 - \\frac{1}{2} (\\frac{x - \\mu_2}{\\sigma_2})^2}$, where $\\frac{1}{\\sigma_1 \\sigma_2 \\sqrt{2\\pi}}$ is constant\n",
    "    - $\\propto e^{-\\frac{1}{2} (\\frac{x - \\mu_1}{\\sigma_1})^2 - \\frac{1}{2} (\\frac{x - \\mu_2}{\\sigma_2})^2} = exp(-\\frac{1}{2}(\\frac{(X_t - \\sqrt{a_t}X_{t-1})^2}{1-a_t} + \\frac{(X_{t-1} - \\sqrt{\\bar a_{t-1}}X_{0})^2}{1- \\bar a_{t-1}}))$\n",
    "- Similarly when we include all three condtional probability:\n",
    "    - $q(X_{t-1}|X_t, X_0) \\propto exp(-\\frac{1}{2}(\\frac{(X_t - \\sqrt{a_t}X_{t-1})^2}{1-a_t} + \\frac{(X_{t-1} - \\sqrt{\\bar a_{t-1}}X_{0})^2}{1- \\bar a_{t-1}} - \\frac{(X_{t} - \\sqrt{\\bar a_t}X_{0})^2}{1- \\bar a_{t}})) $\n",
    "    - I know it's disheartening to see the formula above, but I will do the math for youðŸ˜.\n",
    "    - $\\propto exp(-\\frac{1}{2}(\\frac{X_t^2 - 2\\sqrt{a_t}X_tX_{t-1} + a_tX_{t-1}^2}{1-a_t} + \\frac{X_{t-1}^2 - 2\\sqrt{\\bar a_{t-1}}X_{t-1}X_0 + \\bar a_{t-1}X_0^2}{1- \\bar a_{t-1}} - \\frac{X_{t}^2 - 2\\sqrt{\\bar a_t}X_tX_0 + \\bar a_tX_0^2}{1- \\bar a_{t}}))$\n",
    "    - Since we only want to know the probability of $X_{t-1}$, we pair the terms with $X_{t-1}$ together\n",
    "    - $= exp(-\\frac{1}{2}[(\\frac{a_t}{1-a_t} + \\frac{1}{1-\\bar a_{t-1}})X_{t-1}^2 + (\\frac{2\\sqrt{a_t}X_t}{1-a_t} - \\frac{2\\sqrt{\\bar a_{t-1}}X_0}{1-\\bar a_{t-1}})X_{t-1} + C(X_t, X_0)])$, where $C(X_t, X_0)$ is a term of $X_t$ and $X_0$. Since we don't care about $X_t$ and $X_0$, we treat it as a constant term.\n",
    "    \n",
    "- For any Gaussion distribution, we have:\n",
    "    - $exp(-\\frac{(x - \\mu)^2}{2\\sigma^2}) = exp(-\\frac{1}{2}(\\frac{1}{\\sigma^2}x^2 - \\frac{2\\mu}{\\sigma^2}x + \\frac{\\mu^2}{\\sigma^2}))$\n",
    "\n",
    "- By correspond the terms in the formula above, we can get:\n",
    "    - $\\frac{1}{\\sigma^2} = (\\frac{a_t}{1-a_t} + \\frac{1}{1-\\bar a_{t-1}})$, and $\\beta_t = 1 - a_t$\n",
    "    - $\\frac{1}{\\sigma^2} = \\frac{a_t(1-\\bar a_{t-1}) + \\beta_t}{\\beta_t(1-\\bar a_{t-1})}$\n",
    "    - $\\sigma^2 = \\frac{\\beta_t(1-\\bar a_{t-1})}{a_t(1-\\bar a_{t-1}) + \\beta_t}$\n",
    "\n",
    "- Now we calculate our $\\sigma^2$, the only thing left is the mean $\\mu$ :\n",
    "    - similarly, we have $\\frac{2\\mu}{\\sigma^2} = (\\frac{2\\sqrt{a_t}X_t}{\\beta_t} - \\frac{2\\sqrt{\\bar a_{t-1}}X_0}{1-\\bar a_{t-1}})$\n",
    "    - => $\\mu = \\frac{\\sigma^2}{2}(\\frac{2\\sqrt{a_t}X_t}{\\beta_t} - \\frac{2\\sqrt{\\bar a_{t-1}}X_0}{1-\\bar a_{t-1}})$\n",
    "    - => $\\mu = \\frac{\\beta_t(1-\\bar a_{t-1})}{a_t(1-\\bar a_{t-1}) + \\beta_t}(\\frac{\\sqrt{a_t}X_t(1-\\bar a_{t-1}) - \\beta_t \\sqrt{\\bar a_{t-1}}X_0}{\\beta_t (1-\\bar a_{t-1})})$\n",
    "    - => $\\mu = \\frac{\\sqrt{a_t}X_t(1-\\bar a_{t-1}) - \\beta_t \\sqrt{\\bar a_{t-1}}X_0}{a_t(1-\\bar a_{t-1}) + \\beta_t}$\n",
    "    - => $\\mu = \\frac{\\sqrt{a_t}X_t(1-\\bar a_{t-1})}{a_t(1-\\bar a_{t-1}) + \\beta_t} - \\frac{\\beta_t \\sqrt{\\bar a_{t-1}}}{a_t(1-\\bar a_{t-1}) + \\beta_t}X_0$\n",
    "- take a rest, we are almost there!ðŸ˜…\n",
    "- Subsititute $\\beta_t = 1 - a_t$ into the equation:\n",
    "    - => $\\mu = \\frac{\\sqrt{a_t}(1-\\bar a_{t-1})}{a_t(1-\\bar a_{t-1}) + 1 - a_t}X_t - \\frac{\\beta_t \\sqrt{\\bar a_{t-1}}}{a_t(1-\\bar a_{t-1}) + 1 - a_t}X_0$\n",
    "    - => $\\mu = \\frac{\\sqrt{a_t}(1-\\bar a_{t-1})}{1 - \\bar a_{t-1}}X_t - \\frac{\\beta_t \\sqrt{\\bar a_{t-1}}}{1 - \\bar a_{t-1}}X_0$\n",
    "\n",
    "- But we don't have $X_0$ now, it's the result we want! So we need to get rid of $X_0$ in the equation. \n",
    "    - We can do this by using the formula we derived in the forward process: $X_t = \\sqrt{\\bar a_t}X_{0} + \\sqrt{1-\\bar a_t}Z_t$\n",
    "    - => $\\hat X_0 \\sim X_0 = \\frac{X_t - \\sqrt{1-\\bar a_t}Z_t}{\\sqrt{\\bar a_t}}$\n",
    "\n",
    "- Now our equation would be:\n",
    "    - => $\\mu = \\frac{\\sqrt{a_t}(1-\\bar a_{t-1})}{1 - \\bar a_{t-1}}X_t - \\frac{\\beta_t \\sqrt{\\bar a_{t-1}}}{1 - \\bar a_{t-1}} \\cdot \\frac{X_t - \\sqrt{1-\\bar a_t}Z_t}{\\sqrt{\\bar a_t}}$\n",
    "    - => $\\mu = \\frac{1}{\\sqrt{\\bar a_t}}X_t - \\frac{\\beta_t}{\\sqrt{\\bar a_t}\\sqrt{1-\\bar a_t}} \\cdot Z_t$\n",
    "    - => $\\mu = \\frac{1}{\\sqrt{\\bar a_t}}(X_t - \\frac{\\beta_t}{\\sqrt{1-\\bar a_t}} \\cdot Z_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we got our mean $\\mu$ and variance $\\sigma^2$ for the reverse process. ðŸŽ‰\n",
    "We have:\n",
    "- $\\sigma^2 = \\frac{\\beta_t(1-\\bar a_{t-1})}{a_t(1-\\bar a_{t-1}) + \\beta_t} = \\frac{1- \\bar a_{t-1}}{1- \\bar a_{t}}\\beta_t$\n",
    "- $\\mu = \\frac{1}{\\sqrt{\\bar a_t}}(X_t - \\frac{\\beta_t}{\\sqrt{1-\\bar a_t}} \\cdot Z_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the original paper said?\n",
    "\n",
    "![](mu.png)\n",
    "\n",
    "![](sigma.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hurrah! We have the same results!ðŸŽ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we don't have the $Z_t$ in our $\\mu$, how should we get it? Train Deep Networks! ðŸ˜Ž\n",
    "\n",
    "What model should we use? I will save the answer for the implementation part! ðŸ˜‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In training, we simply use the loss of $Z_t - \\hat Z_t$ or MSE. Where $Z_t$ is from the forward pass, which is the noise added to the image for each timestep. And $\\hat Z_t$ is from the reverse pass, which is the noise predicted by the model.\n",
    "\n",
    "In short, the forward pass provides groud truth for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion for the Math Part ðŸ“\n",
    "\n",
    "From now on NO MORE MATH! ðŸ˜…\n",
    "\n",
    "If you completely lost in the math part, don't worry! You are not gonna believe me, but it doesn't matter! (implementation-wise) ðŸ˜‚\n",
    "\n",
    "The only two equaitions you need to remember are:\n",
    "- $X_t = \\sqrt{\\bar a_t}X_0 + \\sqrt{1 - \\bar a_t}\\bar Z$\n",
    "- $\\mu = \\frac{1}{\\sqrt{\\bar a_t}}(X_t - \\frac{\\beta_t}{\\sqrt{1-\\bar a_t}} \\cdot Z_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the general structure of the model:\n",
    "\n",
    "![](sd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we have the following components:\n",
    "- Encoder\n",
    "- Decoder\n",
    "- Clip Text Encoder\n",
    "- Scheduler\n",
    "- U-Net\n",
    "And we will introduce them one by one in the implementation part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the same diffusion structure from DDPM, the stable diffusion model is actually a lantent  diffusion model which it utilzied VAE encoder to encode the image into a latent space. The VAE encoder is a simple convolutional neural network that takes an image as input and outputs the mean and log variance of the latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, the structure of the VAE encoder is a comprised with a series of convolutional layers, residual blocks,one attention block, and lastly some convolutional layers. In general, the passing through each layer, the original input image is downsampled and the number of channels is increased. The attention block is used to capture the global context of the image. The output of the encoder is the mean and log variance of the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the resisual block structure used in encoder, which the structure is similar to the residual structure in the ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        self.groupnorm_1 = nn.GroupNorm(32, in_channels)\n",
    "        self.conv_1 = nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 1)\n",
    "\n",
    "        self.groupnorm_2 = nn.GroupNorm(32, out_channels)\n",
    "        self.conv_2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, padding = 1)\n",
    "        \n",
    "        # skip connection\n",
    "        if in_channels != out_channels:\n",
    "            self.residual_layer = nn.Conv2d(in_channels, out_channels, kernel_size = 1, padding=0)\n",
    "        else:\n",
    "            self.residual_layer = nn.Identity()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x.shape = (batch_size, in_channels, height, width)\n",
    "        residual = x\n",
    "        x = self.groupnorm_1(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.conv_1(x)\n",
    "        x = self.groupnorm_2(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.conv_2(x)\n",
    "\n",
    "        return x + self.residual_layer(residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the selfAttention block structure used in encoder, which the structure is similar to the selfAttention structure in the Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_heads: int, d_embed: int, in_proj_bias=True, out_proj_bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_proj = nn.Linear(d_embed, 3 * d_embed, bias=in_proj_bias) # Q, K, V\n",
    "        self.out_proj = nn.Linear(d_embed, d_embed, bias=out_proj_bias) # W^O\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_embed // n_heads\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, causal_mask=False) -> torch.Tensor:\n",
    "        # x.shape = (batch_size, seq_len, dim)\n",
    "        input_shape = x.shape\n",
    "        batch_size, seq_len, d_embed = input_shape\n",
    "\n",
    "        intermediate_shape = (batch_size, seq_len, self.n_heads, self.d_head)\n",
    "\n",
    "        # (batch_size, seq_len, dim) -> (batch_size, seq_len, 3 * dim) -> 3 * (batch_size, seq_len, dim)\n",
    "        q, k, v = self.in_proj(x).chunk(3, dim=-1)\n",
    "\n",
    "        # (batch_size, seq_len, dim) -> (batch_size, seq_len, H, Dim/H) -> (batch_size, H, seq_len, Dim/H)\n",
    "        q = q.view(intermediate_shape).transpose(1, 2)\n",
    "        k = k.view(intermediate_shape).transpose(1, 2)\n",
    "        v = v.view(intermediate_shape).transpose(1, 2)\n",
    "\n",
    "        # (batch_size, H, seq_len, Dim/H) x (batch_size, H, Dim/H, seq_len) -> (batch_size, H, seq_len, seq_len)\n",
    "\n",
    "        # Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V\n",
    "        weight = torch.matmul(q, k.transpose(-2, -1))\n",
    "\n",
    "        if causal_mask:\n",
    "            mask = torch.ones_like(weight, dtype=torch.bool).triu(1)\n",
    "            weight.masked_fill_(mask, -torch.inf)\n",
    "        \n",
    "        weight /= math.sqrt(self.d_head)\n",
    "        weight = F.softmax(weight, dim=-1)\n",
    "\n",
    "        # (batch_size, H, seq_len, seq_len) x (batch_size, H, seq_len, Dim/H) -> (batch_size, H, seq_len, Dim/H)\n",
    "        output = torch.matmul(weight, v)\n",
    "        # (batch_size, H, seq_len, Dim/H) -> (batch_size, seq_len, H, Dim/H)\n",
    "        output = output.transpose(1, 2)\n",
    "\n",
    "        # (batch_size, seq_len, H, Dim/H) -> (batch_size, seq_len, Dim)\n",
    "        output = output.reshape(input_shape)\n",
    "\n",
    "        # Multihead(Q, K, V) = Concat(head_1, ..., head_H)W^O\n",
    "        output = self.out_proj(output)\n",
    "        # (batch_size, seq_len, dim)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the attention block uses selfAttention block to capture the global context of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_AttentionBlock(nn.Module):\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        self.groupnorm = nn.GroupNorm(32, channels)\n",
    "        self.attention = SelfAttention(1, channels)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        residual = x\n",
    "        x = self.groupnorm(x)\n",
    "        batch_size, channels, height, width = x.shape\n",
    "\n",
    "        # channels are the features\n",
    "        # (batch_size, channels, height, width) -> (batch_size, channels, height * width)\n",
    "        x = x.view(batch_size, channels, height * width)\n",
    "\n",
    "        # (batch_size, channels, height * width) -> (batch_size, height * width, channels)\n",
    "        x = x.transpose(-1, -2)\n",
    "\n",
    "        # (batch_size, height * width, channels) -> (batch_size, height * width, channels)\n",
    "        x = self.attention(x)\n",
    "\n",
    "        # (batch_size, height * width, channels) -> (batch_size, channels, height * width)\n",
    "        x = x.transpose(-1, -2)\n",
    "\n",
    "        # (batch_size, channels, height * width) -> (batch_size, channels, height, width)\n",
    "        x = x.view((batch_size, channels, height, width))\n",
    "\n",
    "        # x = self.group_norm(x)\n",
    "        x += residual\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual implementation of the VAE encoder is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class VAE_Encoder(nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            # (batch_size, channels, height, width) -> (batch_size, 128, height, width)\n",
    "            nn.Conv2d(3, 128, kernel_size = 3, padding = 1),\n",
    "\n",
    "            VAE_ResidualBlock(128, 128),\n",
    "            VAE_ResidualBlock(128, 128),\n",
    "\n",
    "            # (batch_size, 128, height, width) -> (batch_size, 128, height/2, width/2)\n",
    "            nn.Conv2d(128, 128, kernel_size = 3, stride = 2, padding = 0),\n",
    "\n",
    "            # (batch_size, 128, height/2, width/2) -> (batch_size, 256, height/2, width/2)\n",
    "            VAE_ResidualBlock(128, 256),\n",
    "            VAE_ResidualBlock(256, 256),\n",
    "\n",
    "            # (batch_size, 256, height/2, width/2) -> (batch_size, 256, height/4, width/4)\n",
    "            nn.Conv2d(256, 256, kernel_size = 3, stride = 2, padding = 0),\n",
    "\n",
    "            # (batch_size, 256, height/4, width/4) -> (batch_size, 512, height/4, width/4\n",
    "            VAE_ResidualBlock(256, 512),\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "\n",
    "            # (batch_size, 512, height/4, width/4) -> (batch_size, 512, height/8, width/8)\n",
    "            nn.Conv2d(512, 512, kernel_size = 3, stride = 2, padding = 0),\n",
    "\n",
    "            # (batch_size, 512, height/8, width/8)\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "\n",
    "            # (batch_size, 512, height/8, width/8)\n",
    "            VAE_AttentionBlock(512),\n",
    "            \n",
    "            VAE_ResidualBlock(512, 512),\n",
    "\n",
    "            nn.GroupNorm(32, 512),\n",
    "            nn.SiLU(),\n",
    "\n",
    "            # (batch_size, 512, height/8, width/8) -> (batch_size, 8, height/8, width/8)\n",
    "            # Bottleneck\n",
    "            nn.Conv2d(512, 8, kernel_size = 3, padding = 1),\n",
    "\n",
    "            # (batch_size, 8, height/8, width/8) -> (batch_size, 8, height/8, width/8)\n",
    "            nn.Conv2d(8, 8, kernel_size = 1, padding=0),\n",
    "        \n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, noise = torch.Tensor) -> torch.Tensor:\n",
    "    \n",
    "        for layer in self:\n",
    "            if getattr(layer, 'stride', None) == (2, 2):\n",
    "                # Padding_left, padding_right, padding_top, padding_bottom\n",
    "                x = F.pad(x, (0, 1, 0, 1))\n",
    "            x = layer(x)\n",
    "\n",
    "        # For VAE, want both the mean and the log variance\n",
    "        # (batch_size, 8, height/8, width/8) -> two tensors with size (batch_size, 4, height/8, width/8)\n",
    "        mean, log_variance = x.chunk(2, dim = 1)\n",
    "        # Clamp the log variance to prevent the model from collapsing\n",
    "        log_variance = torch.clamp(log_variance, -30, 20)\n",
    "\n",
    "        # 1/2log(Ïƒ^2) = log(Ïƒ) -> Ïƒ = exp(0.5 * log_variance)\n",
    "        std = (0.5 * log_variance).exp()\n",
    "\n",
    "        # reparameterization trick\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(std)\n",
    "        x = mean + std * noise\n",
    "\n",
    "        # empirical scaling factor\n",
    "        x *= 0.18215\n",
    "\n",
    "        # return the samples from the latent space\n",
    "        # (batch_size, 4, height/8, width/8)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP Text Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO BE CONTINUED..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stableDiffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
